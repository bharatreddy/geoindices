{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "import numpy\n",
    "import time\n",
    "import bs4\n",
    "import urllib\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dst index vals from wdc kyoto website\n",
    "# create a list of dates with monthly freq\n",
    "date_dst_arr = []\n",
    "dst_val = []\n",
    "dst_time_del = datetime.timedelta(hours = 1)\n",
    "start_date = datetime.datetime(2011,1,1)\n",
    "end_date = datetime.datetime(2014,12,31)\n",
    "daterange = pandas.date_range(start_date, end_date, freq=\"M\")\n",
    "for dt in daterange:\n",
    "    if dt.month <= 9:\n",
    "            monthStr = \"0\" + str(dt.month)\n",
    "    else:\n",
    "        monthStr = str(dt.month)\n",
    "    if dt.year >= 2015:\n",
    "        # create the url\n",
    "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_realtime\" + \\\n",
    "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
    "    elif ( (dt.year > 2011) and (dt.year < 2015) ):\n",
    "        # create the url\n",
    "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_provisional\" + \\\n",
    "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
    "    else:\n",
    "        # create the url\n",
    "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_final\" + \\\n",
    "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
    "    conn = urllib.urlopen(currUrl)\n",
    "    htmlSource = conn.read()\n",
    "    soup = bs4.BeautifulSoup(htmlSource, 'html.parser')\n",
    "    dataResObj = soup.find(\"pre\", { \"class\" : \"data\" })\n",
    "    # get the data as a list of strings after removing white space\n",
    "    lines = dataResObj.text.strip().splitlines()\n",
    "    for line in lines[6:]:\n",
    "        columns = line.split()\n",
    "        if len( columns ) > 0. :\n",
    "            date_dst_arr.append( datetime.datetime( \\\n",
    "                dt.year, dt.month, int(columns[0]), 1 ) )\n",
    "            for cols in range( len( columns[1:] ) ) :\n",
    "                try:\n",
    "                    inNumberFloatTest = float(columns[cols + 1])\n",
    "                except:\n",
    "                    # split these cols as well and work on them!\n",
    "                    try:\n",
    "                        missedCols = columns[cols + 1].split(\"-\")[1:]\n",
    "                        if len(missedCols) >= 1:\n",
    "                            for mcols in missedCols:\n",
    "                                dst_val.append( -1*float( mcols ) )\n",
    "                                # now since we added the date earlier we need to be\n",
    "                                # careful about appending date values\n",
    "                                if ( len(date_dst_arr) != len(dst_val) ):\n",
    "                                    date_dst_arr.append ( date_dst_arr[-1] + dst_time_del )\n",
    "                    except:\n",
    "                        print \"something wrong with messed up vals!-->\", columns[cols + 1]\n",
    "                        continue\n",
    "                    continue\n",
    "                # I have to do this because of the messed up way Kyoto puts up the latest dst value..\n",
    "                # mixed with 9999 (fillers) like if latest dst is 1 then Kyoto puts it as 199999.....\n",
    "                if len( columns[ cols + 1 ] ) < 5 :\n",
    "                    dst_val.append( float( columns[ cols + 1 ] ) )\n",
    "                elif ( len( columns[ cols + 1 ] ) > 5 and columns[ cols + 1 ][0:3] != '999' ) :\n",
    "                    mixed_messed_dst = ''\n",
    "                    for jj in range(5) :\n",
    "                        if columns[ cols + 1 ][jj] != '9' :\n",
    "                            mixed_messed_dst = mixed_messed_dst + columns[ cols + 1 ][jj]\n",
    "\n",
    "                    if mixed_messed_dst != '-' :\n",
    "                        dst_val.append( float( mixed_messed_dst ) )\n",
    "                    else :\n",
    "                        dst_val.append( float( 'nan' ) )\n",
    "                else :\n",
    "                    dst_val.append( float( 'nan' ) )\n",
    "                if cols > 0 :\n",
    "                    date_dst_arr.append ( date_dst_arr[-1] + dst_time_del )\n",
    "# convert dst data to a dataframe\n",
    "dstDF = pandas.DataFrame(\n",
    "    {'dst_date': date_dst_arr,\n",
    "     'dst_index': dst_val\n",
    "    })\n",
    "dstDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file containing saps data --> date, time, sapsLat, sapsMLT, sapsVel, radId, poesLat, poesMLT\n",
    "#file_sapsdata = \"/Users/bharat/Desktop/saps-north-2011-2012.txt\"\n",
    "file_sapsdata = \"../data/rawsaps-north-2011-2014.txt\"\n",
    "# store the data to convert it to DF later\n",
    "allData = []\n",
    "# open and read through the file\n",
    "fs = open(file_sapsdata, 'r')\n",
    "# only take data from mid-latitude radars\n",
    "midlatRadIds = [209, 208, 33, 207, 206, 205, 204, 32]\n",
    "for line in fs:\n",
    "    line = line.strip()\n",
    "    columns = line.split()\n",
    "    \n",
    "    dt_ind = time.strptime( columns[0], \"%Y%m%d\" )\n",
    "    hh_ind = int(int(columns[1])/100)\n",
    "    mm_ind = int(int(columns[1]) % 100)\n",
    "    currDt = datetime.datetime( dt_ind.tm_year, dt_ind.tm_mon, dt_ind.tm_mday, hh_ind, mm_ind )\n",
    "    allData.append( [ columns[0] + \"-\" + columns[1], currDt, columns[0], \\\n",
    "                     float( columns[2] ), float( columns[3] ), float( columns[4] ), \\\n",
    "                     float( columns[5] ), float( columns[6] ), float( columns[7] ) ] )  \n",
    "fs.close()\n",
    "# store data in a DF\n",
    "sapsRawDF = pandas.DataFrame(allData)\n",
    "sapsRawDF.columns = [ \"dateTimeString\", \"date\", \"dateStr\", \"sapsLat\", \\\n",
    "                     \"sapsMLT\", \"sapsVel\", \"radId\", \"poesLat\", \"poesMLT\" ]\n",
    "# count number of unique dates present in the raw DF\n",
    "uniqSapsDates = sapsRawDF[\"dateStr\"].unique().tolist()\n",
    "print \"num of unique(total) dates--->\", len(uniqSapsDates)\n",
    "cmprUniqSapsDTStrs = sapsRawDF[\"dateTimeString\"].unique().tolist()\n",
    "sapsRawDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_alldata = \"/home/bharat/Documents/data/newFilt-mid-lat-data-north-2011-2014.txt\"\n",
    "allData = []\n",
    "# open and read through the file\n",
    "fs = open(file_alldata, 'r')\n",
    "for line in fs:\n",
    "    line = line.strip()\n",
    "    columns = line.split()\n",
    "    \n",
    "    dt_ind = time.strptime( columns[0], \"%Y%m%d\" )\n",
    "    hh_ind = int(int(columns[1])/100)\n",
    "    mm_ind = int(int(columns[1]) % 100)\n",
    "    currDt = datetime.datetime( dt_ind.tm_year, dt_ind.tm_mon, dt_ind.tm_mday, hh_ind, mm_ind )\n",
    "    # Only choose SAPS dates\n",
    "    if columns[0] + \"-\" + columns[1] not in cmprUniqSapsDTStrs:\n",
    "        continue\n",
    "    if len(columns) != 9:\n",
    "        print \"wrong!!!-->\", columns\n",
    "        print \"--------------------------------------\"\n",
    "        print line\n",
    "    allData.append( [ columns[0] + \"-\" + columns[1], currDt, columns[0], \\\n",
    "                     float( columns[2] ), float( columns[3] ), float( columns[4] ), \\\n",
    "                     float( columns[5] ), float( columns[6] ), float( columns[7] ), float( columns[8] ) ] )  \n",
    "fs.close()\n",
    "allRawDF = pandas.DataFrame(allData)\n",
    "allRawDF.columns = [ \"dateTimeString\", \"date\", \"dateStr\", \"sapsLat\", \\\n",
    "                     \"sapsMLT\", \"sapsVel\", \"sapsAzim\", \"radId\", \"poesLat\", \"poesMLT\" ]\n",
    "# count number of unique dates present in the raw DF\n",
    "uniqRawDates = allRawDF[\"dateStr\"].unique().tolist()\n",
    "print \"num of unique(total) dates--->\", len(uniqRawDates)\n",
    "allRawDF.head()\n",
    "print \"size of DF--->\", allRawDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead of the daywise thing we'll do an hourwise analysis too!\n",
    "dstDF[\"dateStr\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%Y%m%d'))\n",
    "dstDF[\"hour\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%H'))\n",
    "allRawDF[\"hour\"] = allRawDF[\"date\"].map(lambda x: x.strftime('%H'))\n",
    "allRawDF = pandas.merge( allRawDF, dstDF, on=[\"dateStr\", \"hour\"], how='inner' )\n",
    "fullDatesDstGrps = allRawDF.groupby([\"dateStr\", \"hour\"])\n",
    "minDstFullDataHourwise = fullDatesDstGrps[\"dst_index\"].min()#.aggregate(lambda x: set(tuple(x))).reset_index()\n",
    "# print minDstAllDataHourwise\n",
    "# Below is just a sanity check to verify the joins are working fine\n",
    "# Basically we shouldn't have more than 1 Dst index\n",
    "# minDstFullDataHourwise[\"checkSameDst\"] = minDstFullDataHourwise[\"dst_index\"].map(\\\n",
    "#                                     lambda x: len(x) > 1 )\n",
    "# if (minDstFullDataHourwise[ minDstFullDataHourwise[ \"checkSameDst\" ] == True ][\"dst_index\"].count() == 0):\n",
    "#     print \"JOIN WENT WELL LOOKE FINE!\"\n",
    "# else:\n",
    "#     print \"NEED TO CHECK THE CODE! MAJORRRR PROBLEMMMMM!\"\n",
    "# minDstFullDataHourwise[\"dst_index\"] = minDstFullDataHourwise[\"dst_index\"].map(\\\n",
    "#                                     lambda x: x.pop() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-150,-75,-50,-25,-10,10]\n",
    "fullDataHourWiseBins = pandas.cut( minDstFullDataHourwise, bins=bins )\n",
    "print pandas.value_counts( fullDataHourWiseBins )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poesDF = sapsRawDF[ [ \"dateTimeString\",\"poesLat\",\"poesMLT\" ] ]\n",
    "poesDF.columns = [ \"dateTimeString\",\"poesLat\",\"roundedMLT\" ]\n",
    "poesDF.head()\n",
    "# allRawDF = pandas.merge( allRawDF, poesDF, on=[ \"dateTimeString\", \"sapsMLT\"], how=\"outer\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chngMLTs(row):\n",
    "    if ( (row[\"sapsMLT\"] >= 23.3) ):\n",
    "        return 0.\n",
    "    elif ( (row[\"sapsMLT\"] >= 0.) & (row[\"sapsMLT\"] <= 0.6) ):\n",
    "        return 0.\n",
    "    else:\n",
    "        return round(row[\"sapsMLT\"])\n",
    "allRawDF[\"roundedMLT\"] = allRawDF.apply( chngMLTs, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRawDF = pandas.merge( allRawDF, poesDF, on=[ \"dateTimeString\", \"roundedMLT\"], how=\"outer\" )\n",
    "allRawDF = allRawDF[ [\"dateTimeString\", \"date\", \"dateStr\", \"sapsLat\", \\\n",
    "                     \"sapsMLT\", \"sapsVel\", \"sapsAzim\", \"radId\", \"poesLat_y\", \"roundedMLT\", \"dst_index\"] ]\n",
    "allRawDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRawDF[\"latDiffPOES\"] = allRawDF[\"poesLat_y\"] - allRawDF[\"sapsLat\"]\n",
    "abvPOESCnt = allRawDF[ allRawDF[\"latDiffPOES\"] < 0. ].shape[0]\n",
    "blwPOESCnt = allRawDF[ allRawDF[\"latDiffPOES\"] >= 0. ].shape[0]\n",
    "print \"% SAPS\", blwPOESCnt*100./(abvPOESCnt + blwPOESCnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allRawDF[ (allRawDF[\"sapsAzim\"] >= -105.) & (allRawDF[\"sapsAzim\"] <= -75.) ].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
