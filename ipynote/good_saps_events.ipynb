{
 "metadata": {
  "name": "",
  "signature": "sha256:3beba1a92c11f81c2a049eeab22e769c44e3aa35f5f4b6e783e6204977309f38"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "import datetime\n",
      "import numpy\n",
      "import time\n",
      "import bs4\n",
      "import urllib\n",
      "import seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get dst index vals from wdc kyoto website\n",
      "# create a list of dates with monthly freq\n",
      "date_dst_arr = []\n",
      "dst_val = []\n",
      "dst_time_del = datetime.timedelta(hours = 1)\n",
      "start_date = datetime.datetime(2011,1,1)\n",
      "end_date = datetime.datetime(2014,12,31)\n",
      "daterange = pandas.date_range(start_date, end_date, freq=\"M\")\n",
      "for dt in daterange:\n",
      "    if dt.month <= 9:\n",
      "            monthStr = \"0\" + str(dt.month)\n",
      "    else:\n",
      "        monthStr = str(dt.month)\n",
      "    if dt.year >= 2015:\n",
      "        # create the url\n",
      "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_realtime\" + \\\n",
      "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
      "    elif ( (dt.year > 2011) and (dt.year < 2015) ):\n",
      "        # create the url\n",
      "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_provisional\" + \\\n",
      "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
      "    else:\n",
      "        # create the url\n",
      "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_final\" + \\\n",
      "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
      "    conn = urllib.urlopen(currUrl)\n",
      "    htmlSource = conn.read()\n",
      "    soup = bs4.BeautifulSoup(htmlSource, 'html.parser')\n",
      "    dataResObj = soup.find(\"pre\", { \"class\" : \"data\" })\n",
      "    # get the data as a list of strings after removing white space\n",
      "    lines = dataResObj.text.strip().splitlines()\n",
      "    for line in lines[6:]:\n",
      "        columns = line.split()\n",
      "        if len( columns ) > 0. :\n",
      "            date_dst_arr.append( datetime.datetime( \\\n",
      "                dt.year, dt.month, int(columns[0]), 1 ) )\n",
      "            for cols in range( len( columns[1:] ) ) :\n",
      "                try:\n",
      "                    inNumberFloatTest = float(columns[cols + 1])\n",
      "                except:\n",
      "                    # split these cols as well and work on them!\n",
      "                    try:\n",
      "                        missedCols = columns[cols + 1].split(\"-\")[1:]\n",
      "                        if len(missedCols) >= 1:\n",
      "                            for mcols in missedCols:\n",
      "                                dst_val.append( -1*float( mcols ) )\n",
      "                                # now since we added the date earlier we need to be\n",
      "                                # careful about appending date values\n",
      "                                if ( len(date_dst_arr) != len(dst_val) ):\n",
      "                                    date_dst_arr.append ( date_dst_arr[-1] + dst_time_del )\n",
      "                    except:\n",
      "                        print \"something wrong with messed up vals!-->\", columns[cols + 1]\n",
      "                        continue\n",
      "                    continue\n",
      "                # I have to do this because of the messed up way Kyoto puts up the latest dst value..\n",
      "                # mixed with 9999 (fillers) like if latest dst is 1 then Kyoto puts it as 199999.....\n",
      "                if len( columns[ cols + 1 ] ) < 5 :\n",
      "                    dst_val.append( float( columns[ cols + 1 ] ) )\n",
      "                elif ( len( columns[ cols + 1 ] ) > 5 and columns[ cols + 1 ][0:3] != '999' ) :\n",
      "                    mixed_messed_dst = ''\n",
      "                    for jj in range(5) :\n",
      "                        if columns[ cols + 1 ][jj] != '9' :\n",
      "                            mixed_messed_dst = mixed_messed_dst + columns[ cols + 1 ][jj]\n",
      "\n",
      "                    if mixed_messed_dst != '-' :\n",
      "                        dst_val.append( float( mixed_messed_dst ) )\n",
      "                    else :\n",
      "                        dst_val.append( float( 'nan' ) )\n",
      "                else :\n",
      "                    dst_val.append( float( 'nan' ) )\n",
      "                if cols > 0 :\n",
      "                    date_dst_arr.append ( date_dst_arr[-1] + dst_time_del )\n",
      "# convert dst data to a dataframe\n",
      "dstDF = pandas.DataFrame(\n",
      "    {'dst_date': date_dst_arr,\n",
      "     'dst_index': dst_val\n",
      "    })\n",
      "dstDF.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file containing saps data --> date, time, sapsLat, sapsMLT, sapsVel, radId, poesLat, poesMLT\n",
      "#file_sapsdata = \"/Users/bharat/Desktop/saps-north-2011-2012.txt\"\n",
      "file_sapsdata = \"/Users/bharat/Desktop/rawsaps-north-2011-2014.txt\"\n",
      "# store the data to convert it to DF later\n",
      "allData = []\n",
      "# open and read through the file\n",
      "fs = open(file_sapsdata, 'r')\n",
      "# only take data from mid-latitude radars\n",
      "midlatRadIds = [209, 208, 33, 207, 206, 205, 204, 32]\n",
      "for line in fs:\n",
      "    line = line.strip()\n",
      "    columns = line.split()\n",
      "    \n",
      "    dt_ind = time.strptime( columns[0], \"%Y%m%d\" )\n",
      "    hh_ind = int(int(columns[1])/100)\n",
      "    mm_ind = int(int(columns[1]) % 100)\n",
      "    currDt = datetime.datetime( dt_ind.tm_year, dt_ind.tm_mon, dt_ind.tm_mday, hh_ind, mm_ind )\n",
      "    allData.append( [ columns[0] + \"-\" + columns[1], currDt, columns[0], \\\n",
      "                     float( columns[2] ), float( columns[3] ), float( columns[4] ), \\\n",
      "                     float( columns[5] ), float( columns[6] ), float( columns[7] ) ] )  \n",
      "fs.close()\n",
      "# store data in a DF\n",
      "sapsRawDF = pandas.DataFrame(allData)\n",
      "sapsRawDF.columns = [ \"dateTimeString\", \"date\", \"dateStr\", \"sapsLat\", \\\n",
      "                     \"sapsMLT\", \"sapsVel\", \"radId\", \"poesLat\", \"poesMLT\" ]\n",
      "# count number of unique dates present in the raw DF\n",
      "uniqRawDates = sapsRawDF[\"dateStr\"].unique().tolist()\n",
      "print \"num of unique(total) dates--->\", len(uniqRawDates)\n",
      "sapsRawDF.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a date string and time column for the dst DF\n",
      "dstDF[\"dateStr\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%Y%m%d'))\n",
      "dstDF[\"hour\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%H'))\n",
      "# Make an hour column for the sapsRawDF too\n",
      "sapsRawDF[\"hour\"] = sapsRawDF[\"date\"].map(lambda x: x.strftime('%H'))\n",
      "# Now merge the dst and sapsRaw DFs\n",
      "sapsRawDF = pandas.merge( sapsRawDF, dstDF, on=[\"dateStr\", \"hour\"], how='inner')\n",
      "sapsRawDF.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get dst wise days\n",
      "# count number of (saps) dates present in the processed DF\n",
      "uniqTotalDates = sapsRawDF[\"dateStr\"].unique().tolist()\n",
      "print \"num of saps dates--->\", len(uniqTotalDates)\n",
      "# get daywise min Dst\n",
      "allDatesDstGrps = sapsRawDF.groupby([\"dateStr\"])\n",
      "minDstAllDataDaywise = allDatesDstGrps[\"dst_index\"].min()\n",
      "# We'll use the pandas cut function to bin the data\n",
      "# our bins are --> [-10,10], [-25,-10], [-50,-25], [-75,-50], [-125,-75]\n",
      "bins = [-150,-75,-50,-25,-10,10]\n",
      "allDataDayWiseBins = pandas.cut( minDstAllDataDaywise, bins=bins )\n",
      "print pandas.value_counts( allDataDayWiseBins )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# dates = pandas.DatetimeIndex(sapsRawDF.date)\n",
      "# # get data from only mid-lat radars\n",
      "sapsRawDF = sapsRawDF[ sapsRawDF[\"radId\"].isin(midlatRadIds) ]\n",
      "# sapsRawDF.reset_index(inplace=True, drop=True)\n",
      "sapsDateGrp = sapsRawDF.groupby([\"dateStr\"])\n",
      "numSapsDates = sapsDateGrp[\"dateTimeString\"].count()\n",
      "# # get a list of SAPS dates where num of \n",
      "# # observations is greater than 200.\n",
      "numSapsDates = numSapsDates[ numSapsDates > 200 ]\n",
      "sapsDatesList = numSapsDates.index.tolist()\n",
      "# # get only data points from the selected dates\n",
      "sapsRawDF = sapsRawDF[sapsRawDF[\"dateStr\"].isin(sapsDatesList)].reset_index(drop=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write the data to a file\n",
      "outFileName = \"../data/processedSaps.txt\"\n",
      "# get a df with proper format\n",
      "prcsdSapsDF = sapsRawDF[ [\"dateStr\", \"sapsLat\", \\\n",
      "                     \"sapsMLT\", \"sapsVel\", \"radId\", \"poesLat\", \"poesMLT\", \"dst_date\", \"dst_index\"] ]\n",
      "prcsdSapsDF[\"time\"] = sapsRawDF[\"date\"].map(lambda x: x.strftime('%H%M'))\n",
      "prcsdSapsDF.to_csv(outFileName, sep=' ', index=False)\n",
      "prcsdSapsDF.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count number of (saps) dates present in the processed DF\n",
      "uniqSapsDates = prcsdSapsDF[\"dateStr\"].unique().tolist()\n",
      "print \"num of saps dates--->\", len(uniqSapsDates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get daywise min Dst\n",
      "sapsDFDstGrps = prcsdSapsDF.groupby([\"dateStr\"])\n",
      "minDstDaywise = sapsDFDstGrps[\"dst_index\"].min()\n",
      "# We'll use the pandas cut function to bin the data\n",
      "# our bins are --> [-10,10], [-25,-10], [-50,-25], [-75,-50], [-125,-75]\n",
      "bins = [-150,-75,-50,-25,-10,10]\n",
      "dayWiseBins = pandas.cut( minDstDaywise, bins=bins )\n",
      "print pandas.value_counts(dayWiseBins)\n",
      "print pandas.value_counts( allDataDayWiseBins )\n",
      "# print dayWiseBins\n",
      "# plot a histogram\n",
      "seaborn.distplot(sapsRawDF[\"dst_index\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dstSapsDaysDF = pandas.concat([ \\\n",
      "                     pandas.value_counts( dayWiseBins ), \\\n",
      "                     pandas.value_counts( allDataDayWiseBins )],\\\n",
      "                    axis=1)#.reset_index()\n",
      "dstSapsDaysDF = dstSapsDaysDF.reindex(index = ['(-150, -75]','(-75, -50]',\\\n",
      "                                               '(-50, -25]', '(-25, -10]', '(-10, 10]']).reset_index()\n",
      "dstSapsDaysDF.columns = [ \"dstRange\", \"sapsdays\", \"totaldays\" ]\n",
      "dstSapsDaysDF[\"sapsPercent\"] = dstSapsDaysDF[\"sapsdays\"]*100/dstSapsDaysDF[\"totaldays\"]\n",
      "daysBar = seaborn.barplot( dstSapsDaysDF[\"dstRange\"], dstSapsDaysDF[\"sapsPercent\"], color=\"steelblue\" )\n",
      "plt.ylim(0, 100)\n",
      "plt.xlabel(\"Dst index bins\")\n",
      "plt.ylabel(\"Percent Occurrence\")\n",
      "for n,p in enumerate(daysBar.patches):\n",
      "    titStrg = str( dstSapsDaysDF[\"sapsdays\"][n] ) +\\\n",
      "        \" / \" + str( dstSapsDaysDF[\"totaldays\"][n] )\n",
      "    daysBar.annotate(\n",
      "        s= titStrg,\n",
      "        xy=(p.get_x()+p.get_width()/2.,p.get_height()),\n",
      "        ha='center',va='center',\n",
      "        xytext=(0,10),\n",
      "        textcoords='offset points'\n",
      ")\n",
      "fig = daysBar.get_figure()\n",
      "fig.savefig(\"../figs/dayPercent.pdf\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}